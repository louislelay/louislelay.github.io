<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Louis Le Lay - Personal Website</title>
    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        display: flex;
    }

    #sidebar {
        width: 20%;
        background-color: #333;
        color: white;
        padding: 1%;
        display: flex;
        flex-direction: column;
        align-items: flex-start;
        white-space: nowrap;
        position: fixed;
        height: 100vh;
        overflow-y: auto;
        transition: transform 0.3s ease-in-out;
    }

    #sidebar a {
        color: white !important;
        text-decoration: none;
        padding: 2% 5%;
        display: inline-block;
    }

    #sidebar a:hover {
        background-color: #575757;
    }

    #content {
        flex-grow: 1;
        display: flex;
        flex-direction: column;
        min-height: 100vh;
        transition: transform 0.3s ease-in-out;
        margin-left: 25%;
        margin-right: 10%;
    }

    .section {
        display: none;
        flex-direction: column;
        justify-content: space-between;
        padding-top: 3%;
    }

    .active {
        display: block;
        flex-direction: column;
        justify-content: space-between;
        padding-top: 3%;
    }
    a {
        color: blue !important;
        text-decoration: none;
    }

    .date {
        margin-top: 5%;
        font-weight: bold;
    }

    .repo {
        margin-left: 5%;
        font-weight: bold;
    }

    .pr {
        margin-left: 10%;
    }

    .spacer {
        flex-grow: 1;
    }

    .contact-links a {
        color: blue;
        text-decoration: none;
        margin-right: 3%;
    }
    
    .project-video {
        width: 100%;
        max-width: 80%;
        height: auto;
        aspect-ratio: 16 / 9;
    }

    .image-gallery {
        margin-top: 1em;
        text-align: center; /* Example styling */
        }

    .project-image {
        height: 30vh;
        width: auto;
        /* max-width: 90%; */
        margin-bottom: 1em;
    }

    button.prev,
    button.next {
        background-color: #444;
        color: white;
        border: none;
        padding: 6px 10px;
        margin: 0 5px;
        cursor: pointer;
    }

    button.prev:hover,
    button.next:hover {
        background-color: #666;
    }

    #menu-button {
        display: none;
        background-color: #333;
        color: white;
        border: none;
        padding: 10px 15px;
        font-size: 16px;
        position: fixed;
        top: 10px;
        left: 10px;
        cursor: pointer;
    }

    @media (max-width: 768px) {
        #sidebar {
            transform: translateX(-100%);
            width: 60%;
        }
        
        #sidebar.show {
            transform: translateX(0);
        }

        #menu-button {
            display: block;
        }

        /* #content {
            margin-left: 10%;
        } */
    }

    </style>
</head>
<body>
    <button id="menu-button" onclick="toggleSidebar()">â˜° Menu</button>
    <div id="sidebar">
        <h2>Louis Le Lay</h2>
        <a href="#about" onclick="showSection('about'); closeSidebar();">About Me</a>
        <a href="#background" onclick="showSection('background'); closeSidebar();">Background</a>
        <a href="#projects" onclick="showSection('projects'); closeSidebar();">Projects</a>
        <a href="#opensource" onclick="showSection('opensource'); closeSidebar();">Open Source Contributions</a>
    </div>
    <div id="content">
    <!-- About Me Section -->
    <div id="about" class="section active">
        <h1>About Louis Le Lay</h1>
        <p> Hello! I'm Louis Le Lay, a Robotics Engineering student at Sorbonne University and 
            currently pursuing a dual degree in Information Technology at ETS MontrÃ©al.  
            I specialize in computer vision and simulation with extensive experience 
            in ROS, ROS2, SLAM, and machine learning.
        </p>
        <p>
            Having worked with Airbus on robotic automation and at the University of Tokyo on 
            semantic sensor fusion, I thrive in AI-driven robotics projects that integrate perception and control. 
        </p>
        <h3>ðŸ”§ Core Skills</h3>
        <ul>
            <li><strong>Programming & Tools:</strong> C++, Python, ROS, ROS2, Git, Docker, Gazebo, Mujoco, IsaacSim, IsaacLab, PyBullet, MATLAB</li>
            <li><strong>Core Competencies:</strong> Sensor Fusion, SLAM, VR Simulation, Machine Learning, Reinforcement Learning, Sim2Real Transfer, Control Systems</li>
        </ul>
        <h3>ðŸ“© Contact</h3>
        <p class="contact-links">
            <a href="mailto:le.lay.louis@gmail.com">Email</a> |
            <a href="https://www.linkedin.com/in/louis-le-lay/">LinkedIn</a> |
            <a href="https://github.com/louislelay">GitHub</a>
        </p>
    </div>

    <!-- Background Section -->
    <div id="background" class="section">
        <h1>Background</h1>

        <h2>ðŸ’¼ Experience</h2>

        <h3>International Research Student â€“ University of Tokyo â€“ Tokyo, Japan (Jun 2024 â€“ Aug 2024)</h3>
        <ul>
            <li>Developed ROS-based sensor fusion and calibration packages integrating LiDAR and RGBD data.</li>
            <li>Implemented YOLO-based object detection for enhanced robotic perception.</li>
        </ul>

        <h3>Robotics Engineer Student â€“ Airbus â€“ Paris, France (Aug 2023 â€“ Jun 2024)</h3>
        <ul>
            <li>Designed a robotic proof-of-concept automating A320 cockpit functions.</li>
            <li>Secured executive approval for factory implementation with a projected 13% productivity increase.</li>
        </ul>


        <h2>ðŸŽ“ Education</h2>

        <h3>Master of Engineering â€“ Robotics</h3>
        <ul>
            <li>Sorbonne University â€“ Paris, France (2022 â€“ 2025)</li>
        </ul>

        <h3>Master of Engineering â€“ Information Technology</h3>
        <ul>
            <li>ETS MontrÃ©al â€“ MontrÃ©al, Canada (2024 â€“ 2025)</li>
        </ul>


        <h2>ðŸ“œ Certifications</h2>

        <ul>
            <li>MIT - Machine Learning with Python (Dec 2023)</li>
            <li>NVIDIA - Learn OpenUSD: An Introduction (Nov 2024)</li>
        </ul>
    </div>

    <!-- Projects Section -->
    <div id="projects" class="section">
        <h1>Projects</h1>

        <h2>Disney BD-X Robot Locomotion Training</h2>
        <p>
            Developed a reinforcement learning environment for biped robot locomotion, 
            specifically for the BD-X robot from Disney Research, ensuring compatibility 
            with existing RL frameworks. The environment was designed and implemented with 
            a focus on efficient training and simulation, leading to its successful 
            integration into the IsaacLab repository. 

            <a href="https://github.com/isaac-sim/IsaacLab" target="_blank">{GitHub Repo}</a>
        </p>

        <iframe class="project-video" src="https://www.youtube.com/embed/dT1HWIXhEPs" frameborder="0" allowfullscreen> </iframe>
    

        <h2>VR-Based Throwing Accuracy Study</h2>
        <p>
            Developed a VR simulation to study the impact of visual feedback 
            on throwing accuracy as part of a ball-throwing precision study. 
            The project aimed to investigate skill transfer through VR-based 
            training by building a detailed VR model and conducting experimental 
            analysis. The findings demonstrated the effectiveness of VR in skill 
            training, highlighting its potential for improving motor learning 
            and accuracy. 

            <a href="https://github.com/MPL-projects/vr-project" target="_blank">{GitHub Repo}</a>
        </p>

        <iframe class="project-video" src="https://www.youtube.com/embed/TBcGD_-SzD4" frameborder="0" allowfullscreen></iframe>


        <h2>Bipedal Wheeled Robot</h2>
        <p>
            Developed a wheeled bipedal robot in SolidWorks, focusing on an 
            innovative mechanism that eliminates cantilevering for enhanced 
            stability. After 3D printing the structure, I programmed it using 
            ROS2 for seamless locomotion control. The final design exhibits 
            high agility in diverse environments and demonstrates robust 
            motion capabilities.
            
            <a href="https://github.com/louislelay/Bipedal_Work" target="_blank">GitHub Repo</a>
        </p>
        <!-- Slideshow for multiple images (example) -->
        <div class="image-gallery" data-project="bipedalWheeledRobot">
            <img class="project-image" 
                 src="media/BipedalWheeledRobot/robot-v3-irl.jpeg" 
                 alt="Robot v3 - IRL after 3D Printing" style="display:block;">
            <img class="project-image" 
                 src="media/BipedalWheeledRobot/robot-v3-cad.jpeg" 
                 alt="Robot v2 - Solidworks CAD" style="display:none;">
            <button class="prev" onclick="plusSlides(-1, 'bipedalWheeledRobot')">Previous</button>
            <button class="next" onclick="plusSlides(1, 'bipedalWheeledRobot')">Next</button>
        </div>
    
    
        <h2>Experimental Equipment for Orthostatic Exercise â€“ University of Tokyo</h2>
        <p>
            Created a robotic assistive system to facilitate sit-to-stand motion 
            from a soft chair, primarily targeting elderly individuals. Using 
            MATLAB for both EMG-based intent detection and real-time seat hardening, 
            the project significantly reduced muscle strain, as validated by 
            camera and EMG analyses. 
            
            <a href="https://github.com/louislelay/Experimental-Equipment-for-Orthostatic-Exercise" target="_blank">GitHub Repo</a>
        </p>
        <div class="image-gallery" data-project="orthostaticExercise">
            <img class="project-image"
                 src="media/ExperimentalEquipmentForOrthostaticExercise/equipment.jpeg"
                 alt="Equipment" style="display:block;">
            <img class="project-image"
                 src="media/ExperimentalEquipmentForOrthostaticExercise/equipment-cad.png"
                 alt="CAD of the Equipment" style="display:none;">
            <img class="project-image"
                 src="media/ExperimentalEquipmentForOrthostaticExercise/equipment-plan.png"
                 alt="Plan of the Equipment" style="display:none;">
            <img class="project-image"
                 src="media/ExperimentalEquipmentForOrthostaticExercise/experiment.png"
                 alt="Experiment" style="display:none;">
            <button class="prev" onclick="plusSlides(-1, 'orthostaticExercise')">Previous</button>
            <button class="next" onclick="plusSlides(1, 'orthostaticExercise')">Next</button>
        </div>
    
    
        <h2>Semantic Sensor Fusion for Unitree Go1 â€“ University of Tokyo</h2>
        <p>
            Enhanced the Go1 quadruped robotâ€™s perception by fusing LiDAR and 
            Realsense camera data via ROS, enabling a semantic mapping approach 
            using machine learning. This setup allows the robot to label and 
            interpret its surroundings, supporting tasks like person-following 
            and object-centric SLAM.  
        </p>
        <div class="image-gallery" data-project="semanticGo1">
            <img class="project-image"
                 src="media/SemanticSensorFusionGo1/WuJiaxuPoster-eng.png"
                 alt="Semantic Sensor Fusion Poster (translated to English)" style="display:block;">
            <img class="project-image"
                 src="media/SemanticSensorFusionGo1/fusion-rviz.png"
                 alt="Fusion of the 3D LiDAR and RGB-D data" style="display:none;">
            <img class="project-image"
                 src="media/SemanticSensorFusionGo1/WuJiaxuPoster-jp.jpeg"
                 alt="Semantic Sensor Fusion Poster (Japanese original version)" style="display:none;">
            <button class="prev" onclick="plusSlides(-1, 'semanticGo1')">Previous</button>
            <button class="next" onclick="plusSlides(1, 'semanticGo1')">Next</button>
        </div>
    
    
        <h2>Lane Detection and Vehicle Tracking â€“ Sorbonne University</h2>
        <p>
            Implemented a computer vision pipeline in C++ (OpenCV) that detects road lanes via 
            edge detection and Hough transforms, then identifies and counts vehicles using 
            bounding box tracking. The solution performs reliably under varying conditions, 
            demonstrating robust detection accuracy and efficient vehicle counting. 
            
            <a href="https://github.com/louislelay/Lane-Detection-and-Vehicle-Tracking" target="_blank">GitHub Repo</a>
        </p>
        <div class="image-gallery" data-project="laneDetection">
            <img class="project-image"
                 src="media/LaneDetectionAndVehicleTracking/count.jpeg"
                 alt="Counting the Vehicles" style="display:block;">
            <img class="project-image"
                 src="media/LaneDetectionAndVehicleTracking/steps.jpeg"
                 alt="Steps in Image Analysis" style="display:none;">
            <button class="prev" onclick="plusSlides(-1, 'laneDetection')">Previous</button>
            <button class="next" onclick="plusSlides(1, 'laneDetection')">Next</button>
        </div>
    
    
        <h2>8 DOF Quadruped Robot</h2>
        <p>
            Conceived a low-cost quadruped inspired by Boston Dynamicsâ€™ SPOT and MITâ€™s Cheetah, 
            starting with a 3D-printed leg prototype driven by two servo motors and an Arduino. 
            The current setup lays groundwork for a full 12 DOF upgrade, with plans to 
            implement advanced leg control algorithms for more dynamic movement.
        </p>

        <iframe class="project-video" src="https://www.youtube.com/embed/jixDbttGpBg" frameborder="0" allowfullscreen> </iframe>

        <div class="image-gallery" data-project="8DofQuadruped">
            <img class="project-image"
                 src="media/8DOFQuadruped/quadruped-cad.png"
                 alt="Quadruped Robot CAD- Prototype v1" style="display:block;">
            <img class="project-image"
                 src="media/8DOFQuadruped/leg-cad.png"
                 alt="Quadruped Robot Leg CAD - Prototype v1" style="display:none;">
            <img class="project-image"
                 src="media/8DOFQuadruped/leg-irl-1.jpeg"
                 alt="Quadruped Robot Leg - Prototype v1" style="display:none;">
            <img class="project-image"
                 src="media/8DOFQuadruped/leg-irl-2.jpeg"
                 alt="Quadruped Robot Leg - Prototype v1" style="display:none;">
            <button class="prev" onclick="plusSlides(-1, '8DofQuadruped')">Previous</button>
            <button class="next" onclick="plusSlides(1, '8DofQuadruped')">Next</button>
        </div>
    
    
        <h2>C++ Game â€“ Sorbonne University</h2>
        <p>
            Designed a minimalist 1v1 game on an 8Ã—8 pixel grid entirely in C++, 
            supporting both keyboard and gamepad inputs without relying 
            on any external game engine. This project showcased core game-loop 
            coding, collision handling, and rendering logic in a compact yet 
            fully functional environment.  
            
            <a href="https://github.com/MPL-projects/POO-game" target="_blank">GitHub Repo</a>
        </p>

        <iframe class="project-video" src="https://www.youtube.com/embed/MueVXqSQhI4&" frameborder="0" allowfullscreen> </iframe>

    </div>

    <!-- Open Source Contributions -->
    <!-- </div> -->
    <div id="opensource" class="section">
        <h1>Open Source Contributions</h1>
        <p>My recent contributions to open-source projects:</p>
        <div id="github-prs">Loading...</div>
        <div class="spacer"></div>
        <p> </p>
    </div>
    <!-- </div> -->



    <script>

        // window.addEventListener("load", () => {
        //     unifyAllGalleries(); // or unifyHeightsInGallery(...)
        // });


        function unifyAllGalleries() {
            // Grab all .image-gallery elements
            const allGalleries = document.querySelectorAll('.image-gallery');
            allGalleries.forEach(gallery => {
                unifyHeightsInGallery(gallery);
            });
        }

        function unifyHeightsInGallery(gallery) {
            const images = gallery.querySelectorAll('.project-image');
            let maxHeight = 0;

            // Build an array of Promises so we only measure after images load
            const loadPromises = Array.from(images).map(img => {
                return new Promise(resolve => {
                    if (img.complete) {
                        resolve();
                    } else {
                        img.onload = resolve;
                        img.onerror = resolve;
                    }
                });
            });

            // Once all images in this gallery are loaded, compute the final uniform height
            Promise.all(loadPromises).then(() => {
                // Determine the forced width (80% of gallery container)
                const galleryWidth = gallery.clientWidth;
                const targetWidth = galleryWidth * 0.5;

                // Compute each image's would-be displayed height at that width
                images.forEach(img => {
                    // aspect ratio = naturalHeight / naturalWidth
                    const aspectRatio = img.naturalHeight / img.naturalWidth;
                    const thisDisplayedHeight = targetWidth * aspectRatio;

                    if (thisDisplayedHeight > maxHeight) {
                        maxHeight = thisDisplayedHeight;
                    }
                });

                // Now set all images to that same final width & height
                images.forEach(img => {
                    img.style.width = targetWidth + 'px';
                    img.style.height = maxHeight + 'px';
                    img.style.objectFit = 'contain';  // or 'cover' if you prefer cropping
                });
            });
        }

        // Basic slideshow logic for multiple projects
        function plusSlides(n, projectId) {

            const gallery = document.querySelector(`.image-gallery[data-project="${projectId}"]`);
            const slides = gallery.querySelectorAll('.project-image');
            
            // Find current visible slide index
            let currentIndex = 0;
            for (let i = 0; i < slides.length; i++) {
                if (slides[i].style.display === 'block') {
                    currentIndex = i;
                    break;
                }
            }
    
            // Hide current slide
            slides[currentIndex].style.display = 'none';
    
            // Calculate new index
            let newIndex = currentIndex + n;
            if (newIndex < 0) {
                newIndex = slides.length - 1;
            } else if (newIndex >= slides.length) {
                newIndex = 0;
            }
    
            // Show new slide
            slides[newIndex].style.display = 'block';
        }

        function showSection(id) {
            unifyAllGalleries();

            document.querySelectorAll('.section').forEach(section => {
                section.classList.remove('active');
            });
            document.getElementById(id).classList.add('active');
        }

        function toggleSidebar() {
            document.getElementById('sidebar').classList.toggle('show');
            adjustContentMargin();
        }

        function closeSidebar() {
            document.getElementById('sidebar').classList.remove('show');
            adjustContentMargin();
        }

        function adjustContentMargin() {
            if (window.innerWidth > 768) {
                document.getElementById('content').style.marginLeft = '25%';
            } else {
                document.getElementById('content').style.marginLeft = document.getElementById('sidebar').classList.contains('show') ? '25%' : '10%';
            }
        }

        window.addEventListener('resize', adjustContentMargin);

        async function fetchPullRequests() {
            const response = await fetch('https://api.github.com/search/issues?q=author:louislelay+type:pr');
            const data = await response.json();
            const prContainer = document.getElementById('github-prs');
            prContainer.innerHTML = '';
            
            const groupedPRs = {};
            for (const pr of data.items) {
                if (pr.state === 'closed' && !pr.pull_request.merged_at) continue;
                const date = new Date(pr.created_at).toLocaleDateString();
                const repoName = pr.repository_url.split('/').pop();
                const orgName = pr.repository_url.split('/')[4] || repoName;
                const status = pr.pull_request.merged_at ? 'Merged' : 'Pending';
                const repoLink = `https://github.com/${orgName}/${repoName}`;
                const prNumber = pr.html_url.split('/').pop();
                const starsResponse = await fetch(`https://api.github.com/repos/${orgName}/${repoName}`);
                const repoData = await starsResponse.json();
                const stars = repoData.stargazers_count || 0;
                
                if (!groupedPRs[date]) groupedPRs[date] = {};
                if (!groupedPRs[date][repoName]) groupedPRs[date][repoName] = { orgName, repoLink, stars, prs: [] };
                
                groupedPRs[date][repoName].prs.push({ title: pr.title, status, prNumber, prLink: pr.html_url });
            }
            
            for (const [date, repos] of Object.entries(groupedPRs)) {
                prContainer.innerHTML += `<p class="date">${date}</p>`;
                for (const [repoName, repoData] of Object.entries(repos)) {
                    prContainer.innerHTML += `<p class="repo"><a href="${repoData.repoLink}" target="_blank">${repoData.orgName}/${repoName} (${repoData.stars}â˜…)</a></p><ul>`;
                    repoData.prs.forEach(pr => {
                        prContainer.innerHTML += `<li class="pr">${pr.status} - ${pr.title} <a href="${pr.prLink}" target="_blank">[#${pr.prNumber}]</a></li>`;
                    });
                    prContainer.innerHTML += `</ul>`;
                }
            }
        }
        fetchPullRequests();
    </script>
</body>
</html>
